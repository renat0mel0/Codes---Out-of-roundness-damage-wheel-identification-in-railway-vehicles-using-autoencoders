# -*- coding: utf-8 -*-
"""VAE_Optuna_tempo_segmentado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oH3ImMvrE27u8scZ32MmFbX8YPoUfY9A
"""

!pip install keras==3.1.1
!pip install tensorflow==2.11.0
!pip install optuna
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import scipy.io as sio
import keras
import numpy as np
from google.colab import drive
from keras import backend as K
import seaborn as sns
from scipy.stats import gaussian_kde
import optuna
from sklearn.preprocessing import MinMaxScaler
import os
import re

from google.colab import drive
drive.mount('/content/drive')

# Função para extrair números do nome do arquivo
def ordenar_numericamente(nome_arquivo):
    # Acha todos os números no nome do arquivo
    numeros = re.findall(r'\d+', nome_arquivo)
    # Converte os números encontrados para inteiros e retorna como uma tupla
    return tuple(map(int, numeros))

diretorio1 = '/content/drive/MyDrive/Dados de Corrugação Segmentados/Accel_Rail_Midspan_Lft/1st'
diretorio2 = '/content/drive/MyDrive/Dados de Corrugação Segmentados/Accel_Rail_Midspan_Lft/3rd'
diretorio3 = '/content/drive/MyDrive/Dados de Corrugação Segmentados/Accel_Rail_Midspan_Lft/5th'

arquivos_1 = sorted(os.listdir(diretorio1), key=ordenar_numericamente)
arquivos_2 = sorted(os.listdir(diretorio2), key=ordenar_numericamente)
arquivos_3 = sorted(os.listdir(diretorio3), key=ordenar_numericamente)

print(arquivos_1)
print(arquivos_2)
print(arquivos_3)

# Carregar os dados de Irregularidade 1
corr1 = []
arquivos_corr1 = os.listdir(diretorio1)
for arquivo in arquivos_corr1[:100]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(diretorio1, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['segment'][:,10]
        corr1.append(dados)

corr1 = np.array(corr1)
print(corr1.shape)

plt.figure(figsize=(10, 6))
plt.plot(corr1[0], label='Original')  # Corrigido para massa0[50]
plt.title('Sinal Original - Arquivo 0 - Corrugação 1')
plt.xlabel('Amostras')
plt.ylabel('Valores')
plt.legend()
plt.show()

# Carregar os dados de Irregularidade 2
corr2 = []
arquivos_corr2 = os.listdir(diretorio2)
for arquivo in arquivos_corr2[:100]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(diretorio2, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['segment'][:, 10]
        corr2.append(dados)

corr2 = np.array(corr2)
print(corr2.shape)

plt.figure(figsize=(10, 6))
plt.plot(corr2[0], label='Original')  # Corrigido para massa0[50]
plt.title('Sinal Original - Arquivo 0 - Corrugação 2')
plt.xlabel('Amostras')
plt.ylabel('Valores')
plt.legend()
plt.show()

# Carregar os dados de Irregularidade 3
corr3 = []
arquivos_corr3 = os.listdir(diretorio3)
for arquivo in arquivos_corr3[:100]:
    if arquivo.endswith(".mat"):
        caminho_arquivo = os.path.join(diretorio3, arquivo)
        arquivo_mat = sio.loadmat(caminho_arquivo)
        dados = arquivo_mat['segment'][:, 10]
        corr3.append(dados)

corr3 = np.array(corr3)
print(corr3.shape)

plt.figure(figsize=(10, 6))
plt.plot(corr3[0], label='Original')
plt.title('Sinal Original - Arquivo 0 - Corrugação 3')
plt.xlabel('Amostras')
plt.ylabel('Valores')
plt.legend()
plt.show()

# Calcular a média e o desvio padrão dos dados
media_corr1 = np.mean(corr1)
desvio_corr1 = np.std(corr1)

media_corr2 = np.mean(corr2)
desvio_corr2 = np.std(corr2)

media_corr3 = np.mean(corr3)
desvio_corr3 = np.std(corr3)

# Normalizar
corr1 = (corr1 - media_corr1) / desvio_corr1
corr2 = (corr2 - media_corr2) / desvio_corr2
corr3 = (corr3 - media_corr3) / desvio_corr3

print(corr1.shape)
print(corr2.shape)
print(corr3.shape)

# Função para criar e treinar o VAE
def create_and_train_vae(trial):
    # Parâmetros otimizados
    latent_dim = trial.suggest_int('latent_dim', 150, 500)
    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)
    batch_size = trial.suggest_int('batch_size', 2, 32)
    epochs = trial.suggest_int('epochs', 2, 120)
    intermediate_dim = trial.suggest_int('intermediate_dim', 2000, 2300)

    original_dim = corr1.shape[1]

    inputs = keras.Input(shape=(original_dim,))
    h = keras.layers.Dense(intermediate_dim, activation='relu')(inputs)
    z_mean = keras.layers.Dense(latent_dim)(h)
    z_log_sigma = keras.layers.Dense(latent_dim)(h)

    def sampling(args):
        z_mean, z_log_sigma = args
        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                                  mean=0., stddev=0.1)
        return z_mean + K.exp(0.5*z_log_sigma) * epsilon


    # Número de repetições desejadas
    num_repeticoes = 10

    # Lista para armazenar os erros residuais de cada repetição
    erros_residuais_por_repeticao = []

    # Lista para armazenar ORSR
    erros_orsr_totais = []

    # Lista  para armazenar os erros residuais totais
    erros_residuais_totais = []

    resi_latente_train_acumulado = []
    resi_latente_teste_acumulado = []
    resi_latente_corr2_acumulado = []
    resi_latente_corr3_acumulado = []

    import numpy as np

    for repeticao in range(num_repeticoes):
        print(f'Repetição {repeticao + 1}')

        z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_sigma])

        encoder = tf.keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')

        latent_inputs = tf.keras.Input(shape=(latent_dim,))
        x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)
        outputs = tf.keras.layers.Dense(original_dim, activation='linear')(x)
        decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')

        outputs = decoder(encoder(inputs)[2])
        vae = tf.keras.Model(inputs, outputs, name='vae_mlp')

        reconstruction_loss = tf.reduce_mean(tf.abs(inputs - outputs))
        kl_loss = 1 + z_log_sigma - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_sigma)
        kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)
        kl_loss *= -0.5
        vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)
        vae.add_loss(vae_loss)
        vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))
        # Dividir corr1 em corr1Treino e corr1Teste
        np.random.shuffle(corr1)
        corr1Treino = corr1[:70]
        corr1Teste = corr1[70:]

        # Treinamento do VAE com os dados de Corrugação 1
        vae.fit(corr1Treino, corr1Treino, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=0)

        # Reconstruir os sinais
        corr1_train_rec = vae.predict(corr1Treino)
        corr1_test_rec = vae.predict(corr1Teste)
        corr2_rec = vae.predict(corr2)
        corr3_rec = vae.predict(corr3)

        # Calcular os erros residuais e armazená-los
        erro_train = np.mean(np.square(corr1Treino - corr1_train_rec), axis=1)
        erro_test = np.mean(np.square(corr1Teste - corr1_test_rec), axis=1)
        erro_corr2 = np.mean(np.square(corr2 - corr2_rec), axis=1)
        erro_corr3 = np.mean(np.square(corr3 - corr3_rec), axis=1)
#####################################################################################################################################################################

        erros_residuais = np.concatenate([erro_train, erro_test, erro_corr2, erro_corr3])
        erros_residuais_por_repeticao.append(erros_residuais)

        # Coletar resíduos da camada latente para Corrugação 1 Treino
        resi_latente_train = encoder.predict(corr1Treino)[2]

        # Coletar resíduos da camada latente para Corrugação 1 Teste
        resi_latente_teste = encoder.predict(corr1Teste)[2]

        # Coletar resíduos da camada latente para Corrugação 2
        resi_latente_corr2 = encoder.predict(corr2)[2]

        # Coletar resíduos da camada latente para Corrugação 3
        resi_latente_corr3 = encoder.predict(corr3)[2]

        # Acumular os resíduos da camada latente para Corrugação 1 Treino
        resi_latente_train_acumulado.append(encoder.predict(corr1Treino)[2])

        # Acumular os resíduos da camada latente para Corrugação 1 Teste
        resi_latente_teste_acumulado.append(encoder.predict(corr1Teste)[2])

        # Acumular os resíduos da camada latente para Corrugação 2
        resi_latente_corr2_acumulado.append(encoder.predict(corr2)[2])

        # Acumular os resíduos da camada latente para Corrugação 3
        resi_latente_corr3_acumulado.append(encoder.predict(corr3)[2])

    # Converter as listas acumuladas em matrizes NumPy
    resi_latente_train_acumulado = np.array(resi_latente_train_acumulado)
    resi_latente_teste_acumulado = np.array(resi_latente_teste_acumulado)
    resi_latente_corr2_acumulado = np.array(resi_latente_corr2_acumulado)
    resi_latente_corr3_acumulado = np.array(resi_latente_corr3_acumulado)

    # Calcular a média e o desvio padrão dos resíduos da camada latente para cada classe
    media_resi_latente_corr1 = np.mean(resi_latente_train_acumulado, axis=(0, 1))
    desvio_resi_latente_corr1 = np.std(resi_latente_train_acumulado, axis=(0, 1))
    media_resi_latente_teste_corr1 = np.mean(resi_latente_teste_acumulado, axis=(0, 1))
    desvio_resi_latente_teste_corr1 = np.std(resi_latente_teste_acumulado, axis=(0, 1))
    media_resi_latente_corr2 = np.mean(resi_latente_corr2_acumulado, axis=(0, 1))
    desvio_resi_latente_corr2 = np.std(resi_latente_corr2_acumulado, axis=(0, 1))
    media_resi_latente_corr3 = np.mean(resi_latente_corr3_acumulado, axis=(0, 1))
    desvio_resi_latente_corr3 = np.std(resi_latente_corr3_acumulado, axis=(0, 1))

    # Converter as listas acumuladas em matrizes NumPy
    resi_latente_train_acumulado = np.array(resi_latente_train_acumulado)
    resi_latente_teste_acumulado = np.array(resi_latente_teste_acumulado)
    resi_latente_corr2_acumulado = np.array(resi_latente_corr2_acumulado)
    resi_latente_corr3_acumulado = np.array(resi_latente_corr3_acumulado)

    import numpy as np
    from scipy.stats import f
    import matplotlib.pyplot as plt

    # Parâmetros
    r = 2  # Subgroups com r observações de x (quantidade de sinais)
    m = latent_dim  # Número de características da camada latente

    # Matriz de características da camada latente para treinamento e teste
    features_train = np.concatenate(resi_latente_train_acumulado, axis=0)
    features_test = np.concatenate(resi_latente_teste_acumulado, axis=0)
    features_corr2 = np.concatenate(resi_latente_corr2_acumulado, axis=0)
    features_corr3 = np.concatenate(resi_latente_corr3_acumulado, axis=0)

    # Número de grupos coletados no estado de referência (treinamento) e teste
    s_train = len(resi_latente_train_acumulado)
    s_test = len(resi_latente_teste_acumulado)
    s_corr2 = len(resi_latente_corr2_acumulado)
    s_corr3 = len(resi_latente_corr3_acumulado)

    # Estimador S1:
    S = np.cov(features_train, rowvar=False)  # Matriz de covariância das características da camada latente

    # T-squared para a fase de treinamento
    x_ref_m = np.mean(features_train, axis=0)  # Média dos dados de treinamento
    T_squared_train = []

    for i in range(0, len(features_train), r):
        x = features_train[i:i+r, :]  # "Novos" dados
        x_m = np.mean(x, axis=0)  # Média dos "novos" dados
        T_squared_train.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

    # T-squared para a fase de teste
    T_squared_test = []

    for i in range(0, len(features_test), r):
        x = features_test[i:i+r, :]  # Novos dados
        x_m = np.mean(x, axis=0)  # Média dos novos dados
        T_squared_test.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

    # T-squared para a Corrugação 2
    T_squared_corr2 = []

    for i in range(0, len(features_corr2), r):
        x = features_corr2[i:i+r, :]  # Novos dados
        x_m = np.mean(x, axis=0)  # Média dos novos dados
        T_squared_corr2.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

    # T-squared para a Corrugação 3
    T_squared_corr3 = []

    for i in range(0, len(features_corr3), r):
        x = features_corr3[i:i+r, :]  # Novos dados
        x_m = np.mean(x, axis=0)  # Média dos novos dados
        T_squared_corr3.append(r * np.dot((x_m - x_ref_m), np.dot(np.linalg.pinv(S), (x_m - x_ref_m))))

    fontsize = 20

    # Calcular o percentil 95 dos valores de T_squared_train
    percentile_95 = np.percentile(T_squared_train, 95)

    # Definir a UCL como um múltiplo do percentil 95
    UCL = percentile_95

    # Limite de controle superior para todas as fases (usando apenas os dados de treinamento)
    #F = f.ppf(0.95, m, (s_train * r) - s_train - m + 1)
    #UCL = ((m * (s_train - 1) * (r - 1)) / ((s_train * r) - s_train - m + 1)) * F

    # Calcular a função objetivo
    min_train = np.min(T_squared_train)
    max_val = np.max(T_squared_test)
    objective_value = max_val - min_train

    # Criar uma lista única de UCL para todas as fases
    UCL_axis = np.full(len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2) + len(T_squared_corr3), UCL)

    # Plotar carta de controle T^2 de Hotelling para treinamento, teste, Corrugação 2, Corrugação 3
    plt.figure(figsize=(16, 8))
    plt.plot(np.arange(len(T_squared_train) + len(T_squared_test), len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2)), UCL_axis[len(T_squared_train) + len(T_squared_test):len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2)], 'k', linewidth=3)
    plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2), len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2) + len(T_squared_corr3)), UCL_axis[len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2):len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2) + len(T_squared_corr3)], 'k', linewidth=3)


    plt.plot(np.arange(len(T_squared_train)), T_squared_train, 'x', color='#2924be', label='training')
    plt.plot(np.arange(len(T_squared_train), len(T_squared_train) + len(T_squared_test)), T_squared_test, 'h', color='#71bdd7',fillstyle='none',label='validation')
    plt.plot(np.arange(len(T_squared_train) + len(T_squared_test), len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2)), T_squared_corr2, linestyle='None', marker=(8, 2, 0), markersize=5, mew=0.75,color='#9c1732',label='monitoring')
    plt.plot(np.arange(len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2), len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2) + len(T_squared_corr3)), T_squared_corr3, 'D',color='#82b33d', fillstyle='none', markersize=4,mew=0.75,label='monitoring')
    plt.plot(np.arange(len(T_squared_train)), UCL_axis[:len(T_squared_train)], 'k', linewidth=3, label='UCL')
    plt.plot(np.arange(len(T_squared_train), len(T_squared_train) + len(T_squared_test)), UCL_axis[len(T_squared_train):len(T_squared_train) + len(T_squared_test)], 'k', linewidth=3)

    plt.yscale('log')

    plt.xlabel('data subgroups')
    plt.ylabel('T²')
    plt.legend()
    #plt.title('Carta de Controle T^2 de Hotelling - Treinamento, Teste, irr2, irr3, irr4, irr5, irr6, irr7, irr8, irr9 e irr10')
    #plt.grid()
    plt.xlim(0, len(T_squared_train) + len(T_squared_test) + len(T_squared_corr2) + len(T_squared_corr3))

    # Adicionar linhas verticais tracejadas a cada 200 pontos
    for i in range(0, 1500, 50):
        plt.axvline(x=i, linestyle='--', color='gray', linewidth=1,dashes=(6, 6))

    plt.figure(figsize=(16, 8))
    plt.show()

    # Adicionar a plotagem dos boxplots
    def plot_t_squared_boxplots():
        import matplotlib.pyplot as plt

        # Dados para os boxplots
        data = [T_squared_train, T_squared_test, T_squared_corr2, T_squared_corr3]

        labels = ['Treino', 'Validação', 'CORR2', 'CORR3']

        plt.figure(figsize=(16, 8))
        plt.boxplot(data, labels=labels)
        plt.title('Boxplots dos valores de T-squared para diferentes classes de irregularidades')
        plt.xlabel('Classes de Irregularidades')
        plt.ylabel('Valores de T-squared')
        plt.show()

    # Chamar a função de plotagem dos boxplots
    plot_t_squared_boxplots()

    return objective_value

    #return objective_value

# Otimização e Resultados:
study = optuna.create_study(direction='minimize')
study.optimize(create_and_train_vae, n_trials=50)


# Obter os resultados e os 5 melhores conjuntos de hiperparâmetros
trial_results = study.trials
top5_trials=sorted(trial_results, key=lambda x: x.value, reverse=False)[:5]   #reverse=true faz pegar os MAIORES valores e não os menores.

# Exibir os 5 melhores conjuntos de hiperparâmetros
for i, trial in enumerate(top5_trials, 1):
    print(f"\nTop {i} conjunto de hiperparâmetros:")
    print(f"Trial {trial.number}")
    print(f"Hiperparâmetros: {trial.params}")
    print(f"Valor da função objetivo: {trial.value}")

# Obter os melhores hiperparâmetros
best_params = study.best_params
best_objective_value = study.best_value

print("\nMelhores hiperparâmetros encontrados:")
print(best_params)
print(f"Melhor diferença de erro entre Massa 4 e Massa 3: {best_objective_value}")